{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10bf3438",
   "metadata": {},
   "source": [
    "# Ассистент 1 - LM на основе n-грамм"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de848df",
   "metadata": {},
   "source": [
    "Перед вами - первое дополнительное задание повышенной сложности, в рамках которого вам предстоит начать разработку телеграм-бота с генеративными моделями.\n",
    "\n",
    "Цель данного ноутбука - помочь влиться в разработку ассистента. В данном ноутбуке написан код для \"обучения\" LM на основе n-грамм, для генерации с помощью нее текста, а также сохранение и загрузка модели и токенизатора.\n",
    "\n",
    "Относитесь к данному заданию максимально творчески - любую часть кода можно менять под ваши нужды и желания, можно оптимизировать, добавлять методы генерации, использовать любые данные, обучать сколь угодно \"большую\" модель. \n",
    "\n",
    "При этом вам стоит быть готовыми со всеми техническими проблеми справляться самому - именно так обычно происходит в реальной жизни в реальных проектах :) \n",
    "\n",
    "Поэтому отдельно подчеркну:\n",
    "* если что-то сломалось после ваших изменений - подразумевается, что вы сами найдете проблему и исправите\n",
    "* если вы ничего не трогали, но что-то не работает у нас - подразумевается, что вы сами найдете проблему и исправите :) \n",
    "\n",
    "Главный критерий выполнености данного задания - телеграм-бот, генерирующий текст и использующий обозначенный в задании подход (в случае данного ноутбука - n-граммная модель в любой ее реализации).\n",
    "\n",
    "摆在您面前的是第一个复杂度增加的附加任务，您必须利用生成模型开始开发电报机器人。\n",
    "\n",
    "本手册的目的是帮助您开始开发该助手。在本手册中，我们将编写代码来 \"训练 \"基于 n-grams 的 LM，用它生成文本，并保存和加载模型和标记符。\n",
    "\n",
    "您可以优化代码、添加生成方法、使用任何数据、训练任何 \"大型 \"模型。\n",
    "\n",
    "与此同时，你还应该准备好自己处理所有的技术问题--这就是现实生活中实际项目中通常发生的情况：) \n",
    "\n",
    "因此，我想强调\n",
    "* 如果在您修改后出现了故障--假定您会发现问题并自行修复。\n",
    "* 如果您什么都没动，但有些东西对我们不起作用 - 假定您会自己发现问题并解决它:) \n",
    "\n",
    "本任务的主要标准是电报机器人生成文本并使用任务中指定的方法（就本笔记本而言--n-gram 模型的任何实现方式）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb8bd4",
   "metadata": {},
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca60e8",
   "metadata": {},
   "source": [
    "Для обучения качественной модели вам потребуются датасеты. В ноутбуке составлен маленький игрушечный датасет, вам для улучшения качества потребуется данные в большем количестве и более качественные, а также другие параметры модели и генерации (например, размер контекста побольше). \n",
    "\n",
    "С нормальным датасетом и правильными параметрами даже такой простой моделью можно добиться адекватного качества генерации текста (возможно не очень человечный, но вполне связный текст).\n",
    "\n",
    "Датасеты можно найти и выбрать тут (желательно на русском, вам так будет понятней качество и в целом полезней):\n",
    "https://huggingface.co/datasets\n",
    "  \n",
    "Можете найти наиболее интересный для себя датасет (можете сделать модель как смешной, так и полезной), либо выбрать любой из этих датасетов\n",
    "* https://huggingface.co/datasets/Den4ikAI/russian_dialogues\n",
    "* https://huggingface.co/datasets/Georgii/russianPoetry\n",
    "* https://huggingface.co/datasets/IgorVolochay/russian_jokes\n",
    "\n",
    "要训练出高质量的模型，您需要数据集。笔记本上有一个小的玩具数据集，要提高质量，需要更多和更好的数据，以及模型和生成的其他参数（例如更大的上下文大小）。\n",
    "\n",
    "有了正常的数据集和正确的参数，即使是这样一个简单的模型，也能达到足够的文本生成质量（也许不是非常人性化，但相当连贯的文本）。\n",
    "\n",
    "您可以在这里找到并选择数据集（最好是俄文数据集，这样您可以了解数据集的质量，一般来说也更有用）： https://huggingface.co/datasets。\n",
    "\n",
    "您可以找到自己最感兴趣的数据集（您可以让模型既有趣又有用），也可以从这些数据集中任选一个。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6c33a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T14:50:17.414585Z",
     "iopub.status.busy": "2024-02-21T14:50:17.413838Z",
     "iopub.status.idle": "2024-02-21T14:50:37.187697Z",
     "shell.execute_reply": "2024-02-21T14:50:37.186316Z",
     "shell.execute_reply.started": "2024-02-21T14:50:17.414553Z"
    },
    "ExecuteTime": {
     "end_time": "2024-03-28T14:09:34.202813600Z",
     "start_time": "2024-03-28T14:08:34.952841200Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle \n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import List, Dict, Optional, Iterable, Tuple\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df809daa",
   "metadata": {},
   "source": [
    "Токенизатор разбивает текст на слова. Можно попробовать другие способы токенизации 标记化器会将文本分解成单词。您可以尝试其他标记化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90160389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:11.789756Z",
     "iopub.status.busy": "2024-02-21T15:03:11.788899Z",
     "iopub.status.idle": "2024-02-21T15:03:11.808584Z",
     "shell.execute_reply": "2024-02-21T15:03:11.807519Z",
     "shell.execute_reply.started": "2024-02-21T15:03:11.789725Z"
    },
    "ExecuteTime": {
     "end_time": "2024-03-28T14:09:34.246814Z",
     "start_time": "2024-03-28T14:09:34.214817Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self,\n",
    "                 token_pattern: str = '\\w+|[\\!\\?\\,\\.\\-\\:]',\n",
    "                 eos_token: str = '<EOS>',\n",
    "                 pad_token: str = '<PAD>',\n",
    "                 unk_token: str = '<UNK>'):\n",
    "        self.token_pattern = token_pattern\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "        self.special_tokens = [self.eos_token, self.pad_token, self.unk_token]\n",
    "        self.vocab = None\n",
    "        self.inverse_vocab = None\n",
    "    \n",
    "    def text_preprocess(self, input_text: str) -> str:\n",
    "        \"\"\" Предобрабатываем один текст \"\"\"\n",
    "        # input_text = ... # приведение к нижнему регистру\n",
    "        #input_text = input_text.lower()\n",
    "        input_text = re.sub('\\s+', ' ', input_text) # унифицируем пробелы\n",
    "        input_text = input_text.strip()\n",
    "        return input_text\n",
    "    \n",
    "    def build_vocab(self, corpus: List[str]) -> None:\n",
    "        assert len(corpus)\n",
    "        all_tokens = set()\n",
    "        for text in corpus:\n",
    "            all_tokens |= set(self._tokenize(text, append_eos_token=False))\n",
    "        self.vocab = {elem: ind for ind, elem in enumerate(all_tokens)}\n",
    "        special_tokens = [self.eos_token, self.unk_token, self.pad_token]\n",
    "        for token in special_tokens:\n",
    "            self.vocab[token] = len(self.vocab)\n",
    "        self.inverse_vocab = {ind: elem for elem, ind in self.vocab.items()}\n",
    "        return self\n",
    "        \n",
    "    def _tokenize(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
    "        text = self.text_preprocess(text)\n",
    "        tokens = re.findall(self.token_pattern, text)\n",
    "        if append_eos_token:\n",
    "            tokens.append(self.eos_token)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
    "        \"\"\" Токенизируем текст \"\"\"\n",
    "        tokens = self._tokenize(text, append_eos_token)\n",
    "        ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, input_ids: Iterable[int], remove_special_tokens: bool = False) -> str:\n",
    "        assert len(input_ids)\n",
    "        assert max(input_ids) < len(self.vocab) and min(input_ids) >= 0\n",
    "        tokens = []\n",
    "        for ind in input_ids:\n",
    "            token = self.inverse_vocab[ind]\n",
    "            if remove_special_tokens and token in self.special_tokens:\n",
    "                continue\n",
    "            tokens.append(token)\n",
    "        text = ' '.join( tokens )\n",
    "        return text\n",
    "    \n",
    "    def save(self, path: str) -> bool:\n",
    "        data = {\n",
    "            'token_pattern': self.token_pattern,\n",
    "            'eos_token': self.eos_token,\n",
    "            'pad_token': self.pad_token,\n",
    "            'unk_token': self.unk_token,\n",
    "            'special_tokens': self.special_tokens,\n",
    "            'vocab': self.vocab,\n",
    "            'inverse_vocab': self.inverse_vocab,\n",
    "        }\n",
    "        \n",
    "        with open(path, 'wb') as fout:\n",
    "            pickle.dump(data, fout)\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def load(self, path: str) -> bool:\n",
    "        with open(path, 'rb') as fin:\n",
    "            data = pickle.load(fin)\n",
    "            \n",
    "        self.token_pattern = data['token_pattern']\n",
    "        self.eos_token = data['eos_token']\n",
    "        self.pad_token = data['pad_token']\n",
    "        self.unk_token = data['unk_token']\n",
    "        self.special_tokens = data['special_tokens']\n",
    "        self.vocab = data['vocab']\n",
    "        self.inverse_vocab = data['inverse_vocab']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999f620",
   "metadata": {},
   "source": [
    "Класс для задания параметров генерации, так удобней писать логику для валидации параметров и разные другие доп методы 用于设置生成参数的类，因此更便于编写参数验证逻辑和其他各种附加方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f14d758b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:09:34.246814Z",
     "start_time": "2024-03-28T14:09:34.238813100Z"
    }
   },
   "outputs": [],
   "source": [
    "class GenerationConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Тут можно задать любые параметры и их значения по умолчанию\n",
    "        Значения для стратегии декодирования decoding_strategy: ['max', 'top-p']\n",
    "        您可以在此设置任何参数及其默认值\n",
    "        解码策略的值 decoding_strategy：['max', 'top-p']\n",
    "        \"\"\"\n",
    "        self.temperature = kwargs.pop(\"temperature\", 0.4)\n",
    "        self.max_tokens = kwargs.pop(\"max_tokens\", 32)\n",
    "        self.sample_top_p = kwargs.pop(\"sample_top_p\", 0.3)\n",
    "        self.decoding_strategy = kwargs.pop(\"decoding_strategy\", 'top-p')\n",
    "        self.remove_special_tokens = kwargs.pop(\"remove_special_tokens\", True)\n",
    "        self.validate()\n",
    "        \n",
    "    def validate(self):\n",
    "        \"\"\" Здесь можно валидировать параметры \"\"\"\n",
    "        if not (1.0 > self.sample_top_p > 0):\n",
    "            raise ValueError('sample_top_p')\n",
    "        if self.decoding_strategy not in ['max', 'top-p']:\n",
    "            raise ValueError('decoding_strategy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd923a4",
   "metadata": {},
   "source": [
    "Сама LM на основе n-грамм. Тут используется сглаживание Лапласа (можно поменять на метод backoff при желании), а также есть ряд параметров, сильно влияющий на качество генерации. Один из параметров генерации - стратегия генерации. \n",
    "\n",
    "Когда мы получили вероятности для следующего токена, мы по этим вероятностям хотим выбрать этот следующий токен.\n",
    "\n",
    "Можно просто семплировать из этого распределения - но тогда есть шанс, что будут очень маловероятные токены.\n",
    "\n",
    "Можно брать самый вероятный токен - но это плохо повлияет на разнообразие и \"человечность\" языка\n",
    "\n",
    "Можно воспользовать подходом top-p - семплировать только из тех токенов, которые наиболее вероятны (их вероятности суммируются в заданный p)\n",
    "\n",
    "Можно проверить, что top-p будет генерировать более интересный текст чем max\n",
    "\n",
    "Также обратите внимание на параметр температуры. В случае top-p и семплирования, чем больше делаешь температуру, тем меньше отличаются друг от друга вероятности (распределение стремится к равномерному, даже если исходное распределение имело вполне себе выраженные максимумы), и текст становится более случайным (и разнообразным)\n",
    "\n",
    "LM 本身是基于 n-grams 的。它使用拉普拉斯平滑法（如果需要，也可以改为后退法），有许多参数对生成质量有很大影响。其中一个生成参数就是生成策略。\n",
    "\n",
    "一旦得到下一个标记的概率，我们就可以使用这些概率来选择下一个标记。\n",
    "\n",
    "我们可以简单地从这个分布中抽样，但这样就有可能出现不太可能出现的标记。\n",
    "\n",
    "我们可以选取最有可能的标记--但这会对语言的多样性和 \"人性化 \"产生不良影响。\n",
    "\n",
    "您可以使用 top-p 方法--只对最有可能出现的标记取样（它们的概率总和为给定的 p）。\n",
    "\n",
    "您可以检查一下 top-p 会比 max 产生更多有趣的文本。\n",
    "\n",
    "还要注意温度参数。在 top-p 和采样的情况下，温度越高，概率之间的差异就越小（分布趋于均匀，即使原始分布有相当明显的最大值），文本就会变得更加随机（和多样化）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462316de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:21.268282Z",
     "iopub.status.busy": "2024-02-21T15:03:21.267567Z",
     "iopub.status.idle": "2024-02-21T15:03:21.292501Z",
     "shell.execute_reply": "2024-02-21T15:03:21.291323Z",
     "shell.execute_reply.started": "2024-02-21T15:03:21.268251Z"
    },
    "ExecuteTime": {
     "end_time": "2024-03-28T14:09:34.294825Z",
     "start_time": "2024-03-28T14:09:34.258813900Z"
    }
   },
   "outputs": [],
   "source": [
    "class StatLM:\n",
    "    def __init__(self, \n",
    "                 tokenizer: Tokenizer,\n",
    "                 context_size: int = 2,\n",
    "                 alpha: float = 0.1\n",
    "                ):\n",
    "        \n",
    "        assert context_size >= 1\n",
    "        self.context_size = context_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.n_gramms_stat = defaultdict(int)\n",
    "        self.nx_gramms_stat = defaultdict(int)\n",
    "    \n",
    "\n",
    "    def get_token_by_ind(ind: int) -> str:\n",
    "        return self.tokenizer.vocab.get(ind)\n",
    "    \n",
    "    def get_ind_by_token(token: str) -> int:\n",
    "        return self.tokenizer.inverse_vocab.get(token, self.tokenizer.inverse_vocab[self.unk_token])\n",
    "        \n",
    "    def train(self, train_texts: List[str]):\n",
    "        for sentence in tqdm(train_texts, desc='train lines'):\n",
    "            sentence_ind = self.tokenizer.encode(sentence)\n",
    "            for i in range(len(sentence_ind) - self.context_size):\n",
    "                \n",
    "                seq = tuple(sentence_ind[i: i + self.context_size - 1])\n",
    "                self.n_gramms_stat[seq] += 1\n",
    "                \n",
    "                seq_x = tuple(sentence_ind[i: i + self.context_size])\n",
    "                self.nx_gramms_stat[seq_x] += 1\n",
    "                \n",
    "            seq = tuple(sentence_ind[len(sentence_ind) - self.context_size:])\n",
    "            self.n_gramms_stat[seq] += 1\n",
    "    \n",
    "            # Я добавлю сюда метод Backoff\n",
    "            for ngram in self.nx_gramms_stat.keys():\n",
    "                n_1gram = ngram[:-1]  # Get the (N-1)-gram\n",
    "                if n_1gram not in self.n_gramms_stat:\n",
    "                    # Back-off to lower-order n-gram\n",
    "                    self.nx_gramms_stat[ngram] = self.nx_gramms_stat[ngram[:-1]]\n",
    "            \n",
    "    def sample_token(self, \n",
    "                     token_distribution: np.ndarray,\n",
    "                     generation_config: GenerationConfig) -> int:\n",
    "        if generation_config.decoding_strategy == 'max':\n",
    "            return token_distribution.argmax()\n",
    "        elif generation_config.decoding_strategy == 'top-p':\n",
    "            token_distribution = sorted(list(zip(token_distribution, np.arange(len(token_distribution)))),\n",
    "                                        reverse=True)\n",
    "            total_proba = 0.0\n",
    "            tokens_to_sample = []\n",
    "            tokens_probas = []\n",
    "            for token_proba, ind in token_distribution:\n",
    "                tokens_to_sample.append(ind)\n",
    "                tokens_probas.append(token_proba)\n",
    "                total_proba += token_proba\n",
    "                if total_proba >= generation_config.sample_top_p:\n",
    "                    break\n",
    "            # для простоты отнормируем вероятности, чтобы суммировались в единицу\n",
    "            tokens_probas = np.array(tokens_probas) / generation_config.temperature\n",
    "            tokens_probas = tokens_probas / tokens_probas.sum()\n",
    "            return np.random.choice(tokens_to_sample, p=tokens_probas)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown decoding strategy: {generation_config.decoding_strategy}')\n",
    "            \n",
    "    def save_stat(self, path: str) -> bool:\n",
    "        stat = {\n",
    "            'n_gramms_stat': self.n_gramms_stat,\n",
    "            'nx_gramms_stat': self.nx_gramms_stat,\n",
    "            'context_size': self.context_size,\n",
    "            'alpha': self.alpha\n",
    "        }\n",
    "        with open(path, 'wb') as fout:\n",
    "            pickle.dump(stat, fout)\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def load_stat(self, path: str) -> bool:\n",
    "        with open(path, 'rb') as fin:\n",
    "            stat = pickle.load(fin)\n",
    "            \n",
    "        self.n_gramms_stat = stat['n_gramms_stat']\n",
    "        self.nx_gramms_stat = stat['nx_gramms_stat']\n",
    "        self.context_size = stat['context_size']\n",
    "        self.alpha = stat['alpha']\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def get_stat(self) -> Dict[str, Dict]:\n",
    "        \n",
    "        n_token_stat, nx_token_stat = {}, {}\n",
    "        for token_inds, count in self.n_gramms_stat.items():\n",
    "            n_token_stat[self.tokenizer.decode(token_inds)] = count\n",
    "        \n",
    "        for token_inds, count in self.nx_gramms_stat.items():\n",
    "            nx_token_stat[self.tokenizer.decode(token_inds)] = count\n",
    "        \n",
    "        return {\n",
    "            'n gramms stat': self.n_gramms_stat,\n",
    "            'n+1 gramms stat': self.nx_gramms_stat,\n",
    "            'n tokens stat': n_token_stat,\n",
    "            'n+1 tokens stat': nx_token_stat,\n",
    "        }\n",
    "    \n",
    "    def _get_next_token(self, \n",
    "                        tokens: List[int],\n",
    "                        generation_config: GenerationConfig) -> (int, str):\n",
    "        denominator = self.n_gramms_stat.get(tuple(tokens), 0) + self.alpha * len(self.tokenizer.vocab)\n",
    "        numerators = []\n",
    "        for ind in self.tokenizer.inverse_vocab:\n",
    "            numerators.append(self.nx_gramms_stat.get(tuple(tokens + [ind]), 0) + self.alpha)\n",
    "        \n",
    "        token_distribution = np.array(numerators) / denominator\n",
    "        max_proba_ind = self.sample_token(token_distribution, generation_config)\n",
    "        \n",
    "        next_token = self.tokenizer.inverse_vocab[max_proba_ind]\n",
    "        \n",
    "        return max_proba_ind, next_token\n",
    "            \n",
    "    def generate_token(self, \n",
    "                       text: str, \n",
    "                       generation_config: GenerationConfig\n",
    "                      ) -> Dict:\n",
    "        tokens = self.tokenizer.encode(text, append_eos_token=False) \n",
    "        tokens = tokens[-self.context_size + 1:]\n",
    "        \n",
    "        max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
    "        \n",
    "        return {\n",
    "            'next_token': next_token,\n",
    "            'next_token_num': max_proba_ind,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def generate_text(self, text: str, \n",
    "                      generation_config: GenerationConfig\n",
    "                     ) -> Dict:\n",
    "        \n",
    "        all_tokens = self.tokenizer.encode(text, append_eos_token=False) \n",
    "        tokens = all_tokens[-self.context_size + 1:]\n",
    "        next_token = None\n",
    "        while next_token != self.tokenizer.eos_token and len(all_tokens) < generation_config.max_tokens:   \n",
    "            max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
    "            #print('max_proba_ind, next_token:',max_proba_ind, next_token)\n",
    "            all_tokens.append(max_proba_ind)\n",
    "            tokens = all_tokens[-self.context_size + 1:]\n",
    "        #print('tokenssssss2222222s:',tokens)\n",
    "        new_text = self.tokenizer.decode(all_tokens, generation_config.remove_special_tokens)\n",
    "        #print('new_text~~:',new_text)\n",
    "        finish_reason = 'max tokens'\n",
    "        if all_tokens[-1] == self.tokenizer.vocab[self.tokenizer.eos_token]:\n",
    "            finish_reason = 'end of text'\n",
    "        \n",
    "        return {\n",
    "            'all_tokens': all_tokens,\n",
    "            'total_text': new_text,\n",
    "            'finish_reason': finish_reason\n",
    "        }\n",
    "    \n",
    "    def generate(self, text: str, generation_config: Dict) -> str:\n",
    "        return self.generate_text(text, generation_config)['total_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa25b3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:23.642385Z",
     "iopub.status.busy": "2024-02-21T15:03:23.641563Z",
     "iopub.status.idle": "2024-02-21T15:03:23.647640Z",
     "shell.execute_reply": "2024-02-21T15:03:23.646711Z",
     "shell.execute_reply.started": "2024-02-21T15:03:23.642338Z"
    },
    "ExecuteTime": {
     "end_time": "2024-03-28T14:09:48.798346700Z",
     "start_time": "2024-03-28T14:09:34.282814200Z"
    }
   },
   "outputs": [],
   "source": [
    "#загружаем набор данных\n",
    "from datasets import load_dataset\n",
    "#dataset_jokes = load_dataset(\"IgorVolochay/russian_jokes\")\n",
    "dataset_dial = load_dataset(\"Den4ikAI/russian_dialogues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'relevance'],\n",
      "        num_rows: 2477321\n",
      "    })\n",
      "})\n",
      "как дела?\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dial)\n",
    "#print(dataset_jokes)\n",
    "print(dataset_dial[\"train\"]['question'][0])\n",
    "#print(dataset_jokes[\"train\"]['text'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T14:09:51.818443700Z",
     "start_time": "2024-03-28T14:09:51.726110Z"
    }
   },
   "id": "2127a1f078127159"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['как дела?  там хорошо', 'вы кефир пачему не кушаете, не любите?  я ряженку лучше люблю.', 'если в расходную накладную забить дури и выкурить, то получится приходный документ?  особенно когда придет комиссия проверять документацию', 'покажись в шапке  ды щаз приветик', 'давай не будем об этом  давай поговорим о чем-нибудь другом']\n",
      "2476083\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "texts = []\n",
    "questions = dataset_dial[\"train\"][\"question\"]\n",
    "answers = dataset_dial[\"train\"][\"answer\"]\n",
    "punctuation_set = set(string.punctuation)\n",
    "for question, answer in zip(questions, answers):\n",
    "    if answer is not None and question is not None:  \n",
    "        question = question.lower()\n",
    "        answer = answer.lower()\n",
    "        texts.append(question + \"  \" + answer)\n",
    "#соединили вопросы и ответы из диалога\n",
    "print(texts[:5])  \n",
    "print(len(texts))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T14:10:00.156862800Z",
     "start_time": "2024-03-28T14:09:54.744925900Z"
    }
   },
   "id": "74e01a2277fe8e72"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49521\n",
      "54521\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Поскольку набор данных слишком велик, время обучения модели будет очень большим, \n",
    "# поэтому мы случайным образом выбираем только 0,02 из набора данных.\n",
    "percentage_to_select = 0.02\n",
    "num_samples_to_select = int(len(texts) * percentage_to_select)\n",
    "texts_s = random.sample(texts, num_samples_to_select)\n",
    "print(len(texts_s))\n",
    "#Поскольку алгоритм ngram делает выводы, вычисляя вероятность между N-ым словом и предыдущими N-1.\n",
    "# Поэтому, чтобы модель могла отвечать на вопросы типа \"Кто ты?\" \"Я ассистент llm\", необходимо убедиться, \n",
    "# что набор данных \"кто ты? я ассистент llm .\" встречается достаточное количество раз.\n",
    "\n",
    "for i in range(5000):\n",
    "    texts_s.append('кто ты ? я ассистент llm . ')\n",
    "print(len(texts_s))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T14:10:00.235252700Z",
     "start_time": "2024-03-28T14:10:00.152795600Z"
    }
   },
   "id": "7e1fcda95e0fe90c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common words:\n",
      "Word: и, Count: 23146\n",
      "Word: не, Count: 22323\n",
      "Word: в, Count: 19748\n",
      "Word: а, Count: 15410\n",
      "Word: что, Count: 14950\n",
      "Word: я, Count: 14473\n",
      "Word: на, Count: 10745\n",
      "Word: как, Count: 10570\n",
      "Word: ты, Count: 9768\n",
      "Word: это, Count: 8279\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "#Давайте подсчитаем, какие слова встречаются в наборе данных с высокой частотой\n",
    "def find_most_common_words(data, top_n=5):\n",
    "    word_counter = Counter()\n",
    "    for text in data:\n",
    "        words = text.lower().split()  \n",
    "        word_counter.update(words)\n",
    "    most_common_words = word_counter.most_common(top_n)\n",
    "    return most_common_words\n",
    "\n",
    "top_10_words = find_most_common_words(texts_s, top_n=10)\n",
    "print(\"Top 10 most common words:\")\n",
    "for word, count in top_10_words:\n",
    "    print(f\"Word: {word}, Count: {count}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T14:10:00.483253900Z",
     "start_time": "2024-03-28T14:10:00.215272500Z"
    }
   },
   "id": "b4403bde0088753b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Эта функция напрямую используется в телеграм боте для получения модели и конфига генерации 该功能直接用于电报机器人，以获取模型和配置生成信息"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90f60f19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Обучаем на игрушечных данных 从玩具数据中学习"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a85b767"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для демонстрации того, что происходит, возьмем несколько коротких цитат Джейсона Стэтхема отсюда: 为了说明问题所在，让我们来引用几段杰森-斯坦森（Jason Statham）的简短语录：\n",
    "\n",
    "https://dzen.ru/a/ZRFaGN_gKhX6xTWW"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2674d63a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(temperature = 0.2, max_tokens = 32,\n",
    "                                     sample_top_p = 0.3, decoding_strategy = 'top-p',\n",
    "                                     remove_special_tokens=True)\n",
    "#Поскольку это 3-граммовая модель, генерация нестабильна, поэтому температура и sample_top_p настроены очень низко"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T14:10:00.539259400Z",
     "start_time": "2024-03-28T14:10:00.483253900Z"
    }
   },
   "id": "6b515d5a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98e92a50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:25.218697Z",
     "iopub.status.busy": "2024-02-21T15:03:25.218225Z",
     "iopub.status.idle": "2024-02-21T15:03:25.223271Z",
     "shell.execute_reply": "2024-02-21T15:03:25.222261Z",
     "shell.execute_reply.started": "2024-02-21T15:03:25.218665Z"
    },
    "ExecuteTime": {
     "end_time": "2024-03-28T14:10:01.235043Z",
     "start_time": "2024-03-28T14:10:00.499311500Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().build_vocab(texts_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f61a4e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:25.398291Z",
     "iopub.status.busy": "2024-02-21T15:03:25.397981Z",
     "iopub.status.idle": "2024-02-21T15:03:25.406154Z",
     "shell.execute_reply": "2024-02-21T15:03:25.405134Z",
     "shell.execute_reply.started": "2024-02-21T15:03:25.398265Z"
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-03-28T14:10:01.500132900Z",
     "start_time": "2024-03-28T14:10:01.236053900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'проживая': 0, 'шрамом': 1, 'кредитные': 2, 'занимался': 3, 'уральские': 4}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(tokenizer.vocab.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{':': 51372, '.': 33071, '!': 22875, '?': 72083, ';': 4, ',': 64998, '-': 20643, '<EOS>': 88165, '<UNK>': 88166, '<PAD>': 88167}\n"
     ]
    }
   ],
   "source": [
    "punctuation_dict = {\n",
    "    ':': 0,\n",
    "    '.': 1,\n",
    "    '!': 2,\n",
    "    '?': 3,\n",
    "    ';': 4,\n",
    "    ',': 5,\n",
    "    '-': 5,\n",
    "}\n",
    "#Когда я уже много раз пробовал тренироваться, модель никогда не генерировала знаки препинания\n",
    "# я думал, что это происходит потому, что знаки препинания не хранятся в списке слов tokenizer.vocab.items(), \n",
    "# поэтому вот запрос для всех идентификаторов знаков препинания\n",
    "def contains_punctuation(vocabulary):\n",
    "    for word,_ in vocabulary:\n",
    "        if not word.isalnum():  \n",
    "            punctuation_dict[word]=tokenizer.vocab[word]\n",
    "            # print(f\"знак препинания'{word}' в списке слов \")\n",
    "            # print(tokenizer.vocab[word])\n",
    "    return False\n",
    "\n",
    "contains_punctuation(list(tokenizer.vocab.items()))\n",
    "print(punctuation_dict)\n",
    "#убедились, что знак препинания в списке слов tokenizer.vocab.items()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T14:10:01.592077800Z",
     "start_time": "2024-03-28T14:10:01.504140200Z"
    }
   },
   "id": "3e4e02027e3651fd"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ef0948d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:29.936948Z",
     "iopub.status.busy": "2024-02-21T15:03:29.936563Z",
     "iopub.status.idle": "2024-02-21T15:03:29.957657Z",
     "shell.execute_reply": "2024-02-21T15:03:29.956641Z",
     "shell.execute_reply.started": "2024-02-21T15:03:29.936919Z"
    },
    "ExecuteTime": {
     "end_time": "2024-03-28T15:33:26.560057400Z",
     "start_time": "2024-03-28T14:10:01.536096800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "train lines:   0%|          | 0/54521 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e30a166b8464eda95a605387c73fa0c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# класс, который позволяем строить и использовать языковую модель на основе n-грамм\n",
    "#类，使我们能够建立和使用基于 n-grams 的语言模型\n",
    "stat_lm = StatLM(tokenizer, context_size=3, alpha=0.3) # , sample_top_p = None\n",
    "\n",
    "stat_lm.train(texts_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кто ты ? я ассистент llm . подобен письме самозащиты москвеобл занимательно щука счетное магаз обеляет привычным обижайте вникуда свидетельстве своенравные рпкоманда экспериментов жизненного излечим кисленький беззаботное изгоя покончат яхвисты польются подозреваю\n"
     ]
    }
   ],
   "source": [
    "print(stat_lm.generate(\"кто ты ? \", generation_config))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T16:11:41.268017Z",
     "start_time": "2024-03-28T16:11:39.550111600Z"
    }
   },
   "id": "1c05e8cf1c2eedda"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d054a1d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T15:33:30.048245100Z",
     "start_time": "2024-03-28T15:33:29.696967700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save('models/stat_lm/tokenizer.pkl')\n",
    "stat_lm.save_stat('models/stat_lm/stat_lm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600bcef7",
   "metadata": {},
   "source": [
    "Тут мы для токенизатора сохраняем только спецтокены и словарь, для модели - параметры и статистики n-грамм и n+1-грамм. Потом в телеграм боте подгружаем именно эти параметры\n",
    "\n",
    "在这里，我们只为标记符号生成器保存特殊标记符号和字典，为模型保存 n-grams 和 n+1-grams 的参数和统计数据。然后，我们将这些参数加载到 Telegram 机器人中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf63a9",
   "metadata": {},
   "source": [
    "Когда обучите модель на большом датасете, советую посмотреть на распределение вероятностей для следующего слова при разных входах 当您在大型数据集上训练模型时，我建议您查看以下单词在不同输入情况下的概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e659ff6",
   "metadata": {},
   "source": [
    "### смотрим как конструировать 了解如何构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def construct_model():\n",
    "    config = {\n",
    "        'temperature': 0.3,\n",
    "        'max_tokens': 32,\n",
    "        'sample_top_p': 0.3,\n",
    "        'decoding_strategy': 'top-p',\n",
    "    }\n",
    "\n",
    "    stat_lm_path = 'models/stat_lm/stat_lm.pkl'\n",
    "    tokenizer_path = 'models/stat_lm/tokenizer.pkl'\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.load(tokenizer_path)\n",
    "        \n",
    "    stat_lm = StatLM(tokenizer)\n",
    "    stat_lm.load_stat(stat_lm_path)\n",
    "\n",
    "    generation_config = GenerationConfig(temperature=config['temperature'],\n",
    "                                         max_tokens=config['max_tokens'],\n",
    "                                         sample_top_p=config['sample_top_p'],\n",
    "                                         decoding_strategy=config['decoding_strategy'],\n",
    "                                         remove_special_tokens=True)\n",
    "\n",
    "    kwargs = {'generation_config': generation_config}\n",
    "    print(kwargs)\n",
    "    return stat_lm, kwargs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T15:33:30.076245700Z",
     "start_time": "2024-03-28T15:33:30.052318200Z"
    }
   },
   "id": "4cfdad05"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ad87959",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T15:33:30.380247900Z",
     "start_time": "2024-03-28T15:33:30.068245400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generation_config': <__main__.GenerationConfig object at 0x00000169DE9B6980>}\n"
     ]
    }
   ],
   "source": [
    "model, kwargs = construct_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "517748d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T16:12:26.569877Z",
     "start_time": "2024-03-28T16:12:24.640787500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'кто ты ? я ассистент розмарин черчилль админом негативном причешут мозгу продвижения материнский позвольте облагаться ромбоваться ладнобудем юпитерианкой русопетовки водовка маршрутке предсказание печалят смазливых дурость всяким трещу пояснения расслаблялся полевых вошла валяюсь'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"кто ты? \", **kwargs)"
   ]
  },
  {
   "cell_type": "raw",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "88f588cbcde48b58"
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
