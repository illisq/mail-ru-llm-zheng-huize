{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"daily_dialog\")\n",
    "print(dataset)\n",
    "texts = dataset[\"train\"][\"dialog\"]\n",
    "\n",
    "print(texts[:1])  \n",
    "print(len(texts))\n",
    "# data = []\n",
    "# for text in texts:\n",
    "#     # print(text)\n",
    "#     # print(type(text))\n",
    "#     if len(text) % 2 != 0:\n",
    "#         text = text[:-1]\n",
    "#     paired_list = [(text[i], text[i+1]) for i in range(0, len(text), 2)]\n",
    "#     #print(paired_list)\n",
    "#     data.append(paired_list)\n",
    "# print(data[:3])\n",
    "# print(len(data))\n",
    "import random\n",
    "# Поскольку набор данных слишком велик, время обучения модели будет очень большим, \n",
    "# поэтому мы случайным образом выбираем только 0,02 из набора данных.\n",
    "# percentage_to_select = 0.1\n",
    "# num_samples_to_select = int(len(texts) * percentage_to_select)\n",
    "# texts_s = random.sample(texts, num_samples_to_select)\n",
    "# print(len(texts_s))\n",
    "\n",
    "file_path = \"./data/text.txt\"\n",
    "\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for item in texts:\n",
    "        if len(item) % 2 != 0:\n",
    "            item = item[:-1]\n",
    "        for i in range(len(item)):\n",
    "            if i%2 == 0 :\n",
    "                file.write(\"-%s \" % item[i])\n",
    "            else :\n",
    "                file.write(\"-%s\\n\" % item[i])\n",
    "        #paired_list = [(item[i] + item[i+1]) for i in range(0, len(item), 2)]\n",
    "        #file.write(\"%s\\n\" % paired_list)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 41.11988067626953\n",
      "what are you doing? - Anonymity. Please. I would love to try to understand what you are doing, how you think about this process, the way to deal with the people that you talk to, the reason for the actions you are\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"what are you doing ?\"\n",
    "encoded_input = tokenizer.encode(text, return_tensors='pt')\n",
    "output = model.generate(encoded_input, max_length=50, num_return_sequences=1, do_sample=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(encoded_input, labels=encoded_input)\n",
    "    loss = outputs[0]\n",
    "\n",
    "perplexity = torch.exp(loss).item()\n",
    "\n",
    "print(f'perplexity: {perplexity}')\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T11:27:38.896326700Z",
     "start_time": "2024-04-01T11:27:32.147121600Z"
    }
   },
   "id": "ae773d0a9fadfd32"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 4.673332214355469\n",
      "what are you doing?  - I'm going to have a party this weekend. \n",
      "- Why?  - I want to join my friends for a good night out, so I'll try my best. \n",
      "-Have you seen the\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = './models/gpt'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"what are you doing ?\"\n",
    "encoded_input = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "output = model.generate(encoded_input, max_length=50, num_return_sequences=1, do_sample=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(encoded_input, labels=encoded_input)\n",
    "    loss = outputs[0]\n",
    "\n",
    "perplexity = torch.exp(loss).item()\n",
    "\n",
    "print(f'perplexity: {perplexity}')\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T11:27:43.384377800Z",
     "start_time": "2024-04-01T11:27:41.442893100Z"
    }
   },
   "id": "77733d3a9e21cfa6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Хорошо видно, что perplexity уменьшается. Качество модели улучшилось"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3ca36741c7dd95a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
